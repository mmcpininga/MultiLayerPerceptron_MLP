{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GIT_MultilayerPerceptron_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuSlNrrbXSBf"
      },
      "source": [
        "# **MultiLayer Perceptron - MLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vENa4DwZXqRi"
      },
      "source": [
        "OBJETIVOS:\n",
        "- Implementação da MLP - código base: https://github.com/AdalbertoCq/Deep-Learning-Specialization-Coursera/tree/master/Neural%20Networks%20and%20Deep%20Learning/week4\n",
        "- Implementação da regularização L2 com o objetivo de otimizar o desempenho da MLP. \n",
        "- Variação dos parâmetros: learning_rate, num_iterations e lambda, de modo a verificar o modelo com melhor acurácia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uN3mSzmbLPO"
      },
      "source": [
        "# Importação das bibliotecas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmf8fT49-5hJ"
      },
      "source": [
        "# Definição da função de ativação sigmoide = 1/(1+e(-x))\n",
        "def sigmoid(Z):\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z   \n",
        "    return A, cache\n",
        "\n",
        "# Definição da função de ativação relu = max(0,x)\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "# Definição da derivada da função de ativação relu\n",
        "def relu_backward(dA, cache):\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True)  \n",
        "    dZ[Z <= 0] = 0 # when z<=0, you should set dz to 0 as well. \n",
        "    return dZ\n",
        "\n",
        "# Definição da derivada da função de ativação sigmoide\n",
        "def sigmoid_backward(dA, cache):\n",
        "    Z = cache   \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTlb5RQtfuqt"
      },
      "source": [
        "# **SEM REGULARIZAÇÃO L2** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcWK9eG_ctqo"
      },
      "source": [
        "# Definindo a seed\n",
        "np.random.seed(1)\n",
        "\n",
        "def load_dataset():\n",
        "    '''\n",
        "    Carrega os arquivos train_catvnoncat e test_catvnoncat\n",
        "    Separa em (train e test) e (features e labels)\n",
        "    '''\n",
        "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes   \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
        "\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    '''\n",
        "    Inicializa pesos = aleatório e bias = 1\n",
        "    Arguments:\n",
        "        layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    Returns:\n",
        "        parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    '''\n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)    # number of layers in the network\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))        \n",
        "    return parameters\n",
        "\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    ''' \n",
        "    Implementa a parte linear da propagação direta de uma camada\n",
        "    Arguments:\n",
        "      A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "      W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "      b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    Returns:\n",
        "      Z -- the input of the activation function\n",
        "    '''\n",
        "    Z = np.dot(W, A) + b\n",
        "    cache = (A, W, b)\n",
        "    return Z, cache\n",
        "\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    '''\n",
        "    Implementa a propagação direta para a camada LINEAR-> ATIVAÇÃO\n",
        "    Arguments:\n",
        "      A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "      W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "      b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "      activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    Returns:\n",
        "      A -- the output of the activation function\n",
        "    '''\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)    \n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)    \n",
        "    cache = (linear_cache, activation_cache)\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    '''\n",
        "    Implementa a propagação direta para o cálculo [LINEAR-> RELU] * (L-1) -> LINEAR-> SIGMOID\n",
        "    Arguments:\n",
        "      X -- data, numpy array of shape (input size, number of examples)\n",
        "      parameters -- output of initialize_parameters_deep()\n",
        "    Returns:\n",
        "      AL -- last post-activation value\n",
        "    '''\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2  # number of layers in the neural network\n",
        "    # Implement [LINEAR -> RELU]*(L-1)  \n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    # Implement LINEAR -> SIGMOID\n",
        "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)    \n",
        "    return AL, caches\n",
        "\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    '''\n",
        "    Implementa a função de custo\n",
        "    Arguments:\n",
        "      AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "      Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "    Returns:\n",
        "      cost -- cross-entropy cost\n",
        "    '''\n",
        "    m = Y.shape[1]\n",
        "    cost = (-1/m) * np.sum( np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T) )    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).  \n",
        "    return cost\n",
        "\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    '''\n",
        "    Implementa a parte linear da backpropagation para uma única camada\n",
        "    Arguments:\n",
        "      dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "      cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "    Returns:\n",
        "      dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "      dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "     db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    '''\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
        "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
        "    dA_prev = np.dot(W.T, dZ)    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    '''\n",
        "    Implementa a backpropagation para a camada LINEAR-> ATIVAÇÃO\n",
        "    Arguments:\n",
        "      dA -- post-activation gradient for current layer l\n",
        "      cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "      activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    Returns:\n",
        "      dA_prev -- Gradient of the cost with respect to the activation (of the prchroevious layer l-1), same shape as A_prev\n",
        "      dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "      db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    '''\n",
        "    linear_cache, activation_cache = cache    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache) \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "    '''\n",
        "    Implementa a backpropagation para o grupo [LINEAR-> RELU] * (L-1) -> LINEAR -> SIGMOID\n",
        "    Arguments:\n",
        "      AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "      Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "      caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    Returns:\n",
        "      grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ...\n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ...\n",
        "    '''\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL \n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "\n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "    return grads\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):   \n",
        "    '''\n",
        "    Atualiza parâmetros usando gradiente descendente\n",
        "    Arguments:\n",
        "      parameters -- python dictionary containing your parameters\n",
        "      grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    Returns:\n",
        "      parameters -- python dictionary containing your updated parameters\n",
        "                  parameters[\"W\" + str(l)] = ...\n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    '''\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L): \n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]       \n",
        "    return parameters\n",
        "\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, lamb = 0.5, print_cost=False):#lr was 0.009\n",
        "    '''\n",
        "    Implementa uma rede neural de L camadas: [LINEAR-> RELU] * (L-1) -> LINEAR-> SIGMOID\n",
        "    Arguments:\n",
        "      X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
        "      Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "      layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "      learning_rate -- learning rate of the gradient descent update rule\n",
        "      num_iterations -- number of iterations of the optimization loop\n",
        "      print_cost -- if True, it prints the cost every 100 steps\n",
        "    Returns:\n",
        "      parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    '''\n",
        "    np.random.seed(1)\n",
        "    costs = []                    \n",
        "    # Parameters initialization   \n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID\n",
        "        AL, caches = L_model_forward(X, parameters)  \n",
        "\n",
        "        # Compute cost    \n",
        "        cost = compute_cost(AL, Y)\n",
        "\n",
        "        # Backward propagation\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "\n",
        "        # Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate) \n",
        "\n",
        "        # Print the cost every 100 training example             \n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "\n",
        "def predict(X, y, parameters):   \n",
        "    '''\n",
        "    Previsão dos resultados de uma rede neural de L camadas\n",
        "    Arguments:\n",
        "      X -- data set of examples you would like to label\n",
        "      parameters -- parameters of the trained model\n",
        "    Returns:\n",
        "      p -- predictions for the given dataset X\n",
        "    ''' \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))   \n",
        "\n",
        "    # Forward propagation\n",
        "    probas, _ = L_model_forward(X, parameters)  \n",
        "\n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        if probas[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "    \n",
        "    #print results\n",
        "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))        \n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "id": "lQoFtv9-jTDz",
        "outputId": "693e4aa9-c7cf-4285-d179-486782753cba"
      },
      "source": [
        "# Carregando os datasets\n",
        "train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()\n",
        "\n",
        "# Transformando dimensão do conjunto de treino e teste \n",
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "\n",
        "# Padronizando os dados para valores entre 0 e 1.\n",
        "train_x = train_x_flatten/255.\n",
        "test_x = test_x_flatten/255.\n",
        "print (\"train_x's shape: \" + str(train_x.shape))\n",
        "print (\"test_x's shape: \" + str(test_x.shape))\n",
        "\n",
        "# Definindo o modelo\n",
        "layers_dims = [12288, 20, 7, 5, 1] #  5-layer model\n",
        "parameters = L_layer_model(train_x, train_y, layers_dims, learning_rate = 0.0075, num_iterations = 2500, print_cost = True)\n",
        "\n",
        "# Imprimindo o resultado\n",
        "print(\"Train:\")\n",
        "predictions_train = predict(train_x, train_y, parameters)\n",
        "print(\"Test:\")\n",
        "predictions_test = predict(test_x, test_y, parameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x's shape: (12288, 209)\n",
            "test_x's shape: (12288, 50)\n",
            "Cost after iteration 0: 0.771749\n",
            "Cost after iteration 100: 0.672053\n",
            "Cost after iteration 200: 0.648263\n",
            "Cost after iteration 300: 0.611507\n",
            "Cost after iteration 400: 0.567047\n",
            "Cost after iteration 500: 0.540138\n",
            "Cost after iteration 600: 0.527930\n",
            "Cost after iteration 700: 0.465477\n",
            "Cost after iteration 800: 0.369126\n",
            "Cost after iteration 900: 0.391747\n",
            "Cost after iteration 1000: 0.315187\n",
            "Cost after iteration 1100: 0.272700\n",
            "Cost after iteration 1200: 0.237419\n",
            "Cost after iteration 1300: 0.199601\n",
            "Cost after iteration 1400: 0.189263\n",
            "Cost after iteration 1500: 0.161189\n",
            "Cost after iteration 1600: 0.148214\n",
            "Cost after iteration 1700: 0.137775\n",
            "Cost after iteration 1800: 0.129740\n",
            "Cost after iteration 1900: 0.121225\n",
            "Cost after iteration 2000: 0.113821\n",
            "Cost after iteration 2100: 0.107839\n",
            "Cost after iteration 2200: 0.102855\n",
            "Cost after iteration 2300: 0.100897\n",
            "Cost after iteration 2400: 0.092878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train:\n",
            "Accuracy: 0.9856459330143539\n",
            "Test:\n",
            "Accuracy: 0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_pnC2bXdmGO"
      },
      "source": [
        "# **RESULTADOS SEM REGULARIZAÇÃO:**\n",
        "Com learning_rate = 0.0075 e num_iterations = 2500 a acurácia foi:\n",
        "- Accuracy train: 0.9856459330143539\n",
        "- Accuracy test: 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR8cPxZlhCSr"
      },
      "source": [
        "# **COM REGULARIZAÇÃO L2** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds_jxr0muCVN"
      },
      "source": [
        "# Definindo a seed\n",
        "np.random.seed(1)\n",
        "\n",
        "def load_dataset():\n",
        "    '''\n",
        "    Carrega os arquivos train_catvnoncat e test_catvnoncat\n",
        "    Separa em (train e test) e (features e labels)\n",
        "    '''\n",
        "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes   \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
        "\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    '''\n",
        "    Inicializa pesos = aleatório e bias = 1\n",
        "    Arguments:\n",
        "        layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    Returns:\n",
        "        parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    '''\n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)    # number of layers in the network\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))        \n",
        "    return parameters\n",
        "\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    ''' \n",
        "    Implementa a parte linear da propagação direta de uma camada\n",
        "    Arguments:\n",
        "      A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "      W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "      b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    Returns:\n",
        "      Z -- the input of the activation function\n",
        "    '''\n",
        "    Z = np.dot(W, A) + b\n",
        "    cache = (A, W, b)\n",
        "    return Z, cache\n",
        "\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    '''\n",
        "    Implementa a propagação direta para a camada LINEAR-> ATIVAÇÃO\n",
        "    Arguments:\n",
        "      A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "      W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "      b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "      activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    Returns:\n",
        "      A -- the output of the activation function\n",
        "    '''\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)    \n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)    \n",
        "    cache = (linear_cache, activation_cache)\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    '''\n",
        "    Implementa a propagação direta para o cálculo [LINEAR-> RELU] * (L-1) -> LINEAR-> SIGMOID\n",
        "    Arguments:\n",
        "      X -- data, numpy array of shape (input size, number of examples)\n",
        "      parameters -- output of initialize_parameters_deep()\n",
        "    Returns:\n",
        "      AL -- last post-activation value\n",
        "    '''\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2  # number of layers in the neural network\n",
        "    # Implement [LINEAR -> RELU]*(L-1)  \n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    # Implement LINEAR -> SIGMOID\n",
        "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)    \n",
        "    return AL, caches\n",
        "\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    '''\n",
        "    Implementa a função de custo\n",
        "    Arguments:\n",
        "      AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "      Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "    Returns:\n",
        "      cost -- cross-entropy cost\n",
        "    '''\n",
        "    m = Y.shape[1]\n",
        "    cost = (-1/m) * np.sum( np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T) )    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).  \n",
        "    return cost\n",
        "\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    '''\n",
        "    Implementa a parte linear da backpropagation para uma única camada\n",
        "    Arguments:\n",
        "      dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "      cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "    Returns:\n",
        "      dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "      dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "     db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    '''\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
        "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
        "    dA_prev = np.dot(W.T, dZ)    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    '''\n",
        "    Implementa a backpropagation para a camada LINEAR-> ATIVAÇÃO\n",
        "    Arguments:\n",
        "      dA -- post-activation gradient for current layer l\n",
        "      cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "      activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    Returns:\n",
        "      dA_prev -- Gradient of the cost with respect to the activation (of the prchroevious layer l-1), same shape as A_prev\n",
        "      dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "      db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    '''\n",
        "    linear_cache, activation_cache = cache    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache) \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "    '''\n",
        "    Implementa a backpropagation para o grupo [LINEAR-> RELU] * (L-1) -> LINEAR -> SIGMOID\n",
        "    Arguments:\n",
        "      AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "      Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "      caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    Returns:\n",
        "      grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ...\n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ...\n",
        "    '''\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL \n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "\n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "    return grads\n",
        "\n",
        "\n",
        "def update_parameters(X, parameters, grads, learning_rate, lamb):   \n",
        "    '''\n",
        "    Atualiza parâmetros usando gradiente descendente\n",
        "    Arguments:\n",
        "      X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
        "      parameters -- python dictionary containing your parameters\n",
        "      grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "      learning_rate -- learning rate of the gradient descent update rule\n",
        "      lamb -- lamda: L2 regularization\n",
        "    Returns:\n",
        "      parameters -- python dictionary containing your updated parameters\n",
        "                  parameters[\"W\" + str(l)] = ...\n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    '''\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L): #com regularization l2\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * lamb * parameters[\"W\" + str(l+1)]/(2*m) - learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]      \n",
        "    return parameters\n",
        "\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, lamb = 0.5, print_cost=False):#lr was 0.009\n",
        "    '''\n",
        "    Implementa uma rede neural de L camadas: [LINEAR-> RELU] * (L-1) -> LINEAR-> SIGMOID\n",
        "    Arguments:\n",
        "      X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
        "      Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "      layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "      learning_rate -- learning rate of the gradient descent update rule\n",
        "      num_iterations -- number of iterations of the optimization loop\n",
        "      print_cost -- if True, it prints the cost every 100 steps\n",
        "    Returns:\n",
        "      parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    '''\n",
        "    np.random.seed(1)\n",
        "    costs = []                    \n",
        "    # Parameters initialization   \n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID\n",
        "        AL, caches = L_model_forward(X, parameters)  \n",
        "\n",
        "        # Compute cost    \n",
        "        cost = compute_cost(AL, Y)\n",
        "\n",
        "        # Backward propagation\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "\n",
        "        # Update parameters\n",
        "        parameters = update_parameters(X, parameters, grads, learning_rate, lamb) \n",
        "\n",
        "        # Print the cost every 100 training example             \n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "\n",
        "def predict(X, y, parameters):   \n",
        "    '''\n",
        "    Previsão dos resultados de uma rede neural de L camadas\n",
        "    Arguments:\n",
        "      X -- data set of examples you would like to label\n",
        "      parameters -- parameters of the trained model\n",
        "    Returns:\n",
        "      p -- predictions for the given dataset X\n",
        "    ''' \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))   \n",
        "\n",
        "    # Forward propagation\n",
        "    probas, _ = L_model_forward(X, parameters)  \n",
        "\n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        if probas[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "    \n",
        "    #print results\n",
        "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))        \n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OHDG_lSuu40M",
        "outputId": "7cfe8dac-2443-481c-906d-dc4ae29ed1c0"
      },
      "source": [
        "train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()\n",
        "\n",
        "# Reshape the training and test examples \n",
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "train_x = train_x_flatten/255.\n",
        "test_x = test_x_flatten/255.\n",
        "print (\"train_x's shape: \" + str(train_x.shape))\n",
        "print (\"test_x's shape: \" + str(test_x.shape))\n",
        "\n",
        "# Definindo o modelo\n",
        "layers_dims = [12288, 20, 7, 5, 1] #  5-layer model\n",
        "parameters = L_layer_model(train_x, train_y, layers_dims, learning_rate = 0.0075, num_iterations = 5000, lamb = 2, print_cost = True)\n",
        "\n",
        "# Imprimindo o resultado\n",
        "print(\"Train:\")\n",
        "predictions_train = predict(train_x, train_y, parameters)\n",
        "print(\"Test:\")\n",
        "predictions_test = predict(test_x, test_y, parameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x's shape: (12288, 209)\n",
            "test_x's shape: (12288, 50)\n",
            "Cost after iteration 0: 0.771749\n",
            "Cost after iteration 100: 0.672097\n",
            "Cost after iteration 200: 0.646818\n",
            "Cost after iteration 300: 0.609343\n",
            "Cost after iteration 400: 0.564448\n",
            "Cost after iteration 500: 0.569493\n",
            "Cost after iteration 600: 0.466507\n",
            "Cost after iteration 700: 0.463403\n",
            "Cost after iteration 800: 0.388323\n",
            "Cost after iteration 900: 0.330517\n",
            "Cost after iteration 1000: 0.263441\n",
            "Cost after iteration 1100: 0.263524\n",
            "Cost after iteration 1200: 0.223266\n",
            "Cost after iteration 1300: 0.188250\n",
            "Cost after iteration 1400: 0.160406\n",
            "Cost after iteration 1500: 0.140377\n",
            "Cost after iteration 1600: 0.126555\n",
            "Cost after iteration 1700: 0.113872\n",
            "Cost after iteration 1800: 0.103356\n",
            "Cost after iteration 1900: 0.095677\n",
            "Cost after iteration 2000: 0.088349\n",
            "Cost after iteration 2100: 0.082132\n",
            "Cost after iteration 2200: 0.077046\n",
            "Cost after iteration 2300: 0.074257\n",
            "Cost after iteration 2400: 0.068199\n",
            "Cost after iteration 2500: 0.065663\n",
            "Cost after iteration 2600: 0.062009\n",
            "Cost after iteration 2700: 0.058535\n",
            "Cost after iteration 2800: 0.056206\n",
            "Cost after iteration 2900: 0.053793\n",
            "Cost after iteration 3000: 0.052741\n",
            "Cost after iteration 3100: 0.050264\n",
            "Cost after iteration 3200: 0.048492\n",
            "Cost after iteration 3300: 0.047399\n",
            "Cost after iteration 3400: 0.046656\n",
            "Cost after iteration 3500: 0.045719\n",
            "Cost after iteration 3600: 0.043637\n",
            "Cost after iteration 3700: 0.043622\n",
            "Cost after iteration 3800: 0.041607\n",
            "Cost after iteration 3900: 0.041930\n",
            "Cost after iteration 4000: 0.040574\n",
            "Cost after iteration 4100: 0.039454\n",
            "Cost after iteration 4200: 0.039295\n",
            "Cost after iteration 4300: 0.038641\n",
            "Cost after iteration 4400: 0.037766\n",
            "Cost after iteration 4500: 0.037662\n",
            "Cost after iteration 4600: 0.037071\n",
            "Cost after iteration 4700: 0.037477\n",
            "Cost after iteration 4800: 0.036122\n",
            "Cost after iteration 4900: 0.036115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train:\n",
            "Accuracy: 0.9952153110047844\n",
            "Test:\n",
            "Accuracy: 0.8400000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXblwtjyTxuQ"
      },
      "source": [
        "# **RESULTADOS COM REGULARIZAÇÃO:**\n",
        "\n",
        "\n",
        "##Modificando a learning rate para: 0.0075, 0.0005, 0.009 e 0.007\n",
        "##Fixos: num_iterations = 3000 e lamb = 2\n",
        "\n",
        "parameters: learning_rate = 0.0075, num_iterations = 3000, lamb = 2\n",
        "- Accuracy train: 0.9952153110047844\n",
        "- Accuracy test: 0.8200000000000001\n",
        "\n",
        "parameters: learning_rate = 0.0005, num_iterations = 3000, lamb = 2\n",
        "- Accuracy train:0.7990430622009568\n",
        "- Accuracy test: 0.5\n",
        "\n",
        "parameters: learning_rate = 0.009, num_iterations = 3000, lamb = 2\n",
        "- Accuracy train: 0.9952153110047844\n",
        "- Accuracy test: 0.8\n",
        "\n",
        "parameters: learning_rate = 0.007, num_iterations = 3000, lamb = 2\n",
        "- Accuracy train:: 0.9952153110047846\n",
        "- Accuracy test: 0.78\n",
        "\n",
        "Após variar o valor da learning rate, notou-se que ao aumentar muito seu valor, a acurácia diminuiu. Da mesma forma, ao diminuir o valor da larning rate a acurácia também diminuiu. Portanto, fixou-se a learning rate em 0.0075 que tem a melhor acurácia.\n",
        "\n",
        "##Modificando a lambda para: 0.5, 1 , 2 e 3\n",
        "##Fixos: learning_rate = 0.0075 e num_iterations = 3000\n",
        "\n",
        "parameters: learning_rate = 0.0075, num_iterations = 3000, lamb = 2\n",
        "- Accuracy train: 0.9952153110047844\n",
        "- Accuracy test: 0.8200000000000001\n",
        "\n",
        "parameters: learning_rate = 0.0075, num_iterations = 3000, lamb = 3\n",
        "- Accuracy train: 0.9999999999999998\n",
        "- Accuracy test: 0.76\n",
        "\n",
        "parameters: learning_rate = 0.0075, num_iterations = 3000, lamb = 1\n",
        "- Accuracy train:0.9856459330143539\n",
        "- Accuracy test: 0.8200000000000001\n",
        "\n",
        "parameters: learning_rate = 0.0075, num_iterations = 3000, lamb = 0.5\n",
        "- Accuracy train: 0.9904306220095691\n",
        "- Accuracy test: 0.76\n",
        "\n",
        "Após variar o valor do lambda, notou-se que os valores de lambda = 1 ou 2 possuem melhores valores da acurácia. Portanto, fixou-se a lambda em 2.\n",
        "\n",
        "##Modificando o num de interações para: 3500, 4000, 5000 e 10000\n",
        "##Fixos: learning_rate = 0.0075 e lamb = 2\n",
        "\n",
        "parameters: learning_rate = 0.0075, num_iterations = 3500, lamb = 2\n",
        "- Accuracy train: 0.9952153110047844\n",
        "- Accuracy test: 0.8200000000000001\n",
        "\n",
        "parameters: learning_rate = 0.0075, num_iterations = 4000, lamb = 2\n",
        "- Accuracy train: 0.9952153110047844\n",
        "- Accuracy test: 0.8200000000000001\n",
        "\n",
        "parameters: learning_rate = 0.0075, num_iterations = 5000, lamb = 2\n",
        "- Accuracy train: 0.9952153110047844\n",
        "- Accuracy test: 0.8400000000000001\n",
        "\n",
        "parameters: learning_rate = 0.0075, num_iterations = 10000, lamb = 2\n",
        "- Accuracy train: 0.9952153110047844\n",
        "- Accuracy test: 0.8200000000000001\n",
        "\n",
        "Por último, o número de interações foi variado e notou-se que o valor de 5000 interações possui melhor acurácia. Portanto o modelo final permaneceu com os seguintes parâmetros: learning_rate = 0.0075, num_iterations = 5000 e lamb = 2 \n",
        "\n",
        "Melhor acurácia:\n",
        "- Accuracy train: 0.9952153110047844\n",
        "- Accuracy test: 0.8400000000000001"
      ]
    }
  ]
}